<!DOCTYPE html>
<html lang="ko-kr">
  <head>
    
    <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="generator" content="Hugo 0.65.0 with theme Tranquilpeak 0.4.8-BETA">
<meta name="author" content="Deumji Woo">
<meta name="keywords" content="">
<meta name="description" content="[Paper review] TimeSformer
title   = Is Space-Time Attention All You Need for Video Understanding?
author  = Gedas Bertasius and Heng Wang and Lorenzo Torresani
year    = 9 Feb 2021">


<meta property="og:description" content="[Paper review] TimeSformer
title   = Is Space-Time Attention All You Need for Video Understanding?
author  = Gedas Bertasius and Heng Wang and Lorenzo Torresani
year    = 9 Feb 2021">
<meta property="og:type" content="article">
<meta property="og:title" content="[Paper review] TimeSformer">
<meta name="twitter:title" content="[Paper review] TimeSformer">
<meta property="og:url" content="https://dj-woo.github.io/2021/02/paper-review-timesformer/">
<meta property="twitter:url" content="https://dj-woo.github.io/2021/02/paper-review-timesformer/">
<meta property="og:site_name" content="djwoo blog">
<meta property="og:description" content="[Paper review] TimeSformer
title   = Is Space-Time Attention All You Need for Video Understanding?
author  = Gedas Bertasius and Heng Wang and Lorenzo Torresani
year    = 9 Feb 2021">
<meta name="twitter:description" content="[Paper review] TimeSformer
title   = Is Space-Time Attention All You Need for Video Understanding?
author  = Gedas Bertasius and Heng Wang and Lorenzo Torresani
year    = 9 Feb 2021">
<meta property="og:locale" content="en-us">

  
    <meta property="article:published_time" content="2021-02-25T14:15:19">
  
  
    <meta property="article:modified_time" content="2021-02-25T14:15:19">
  
  
  
    
      <meta property="article:section" content="paper review">
    
  
  
    
      <meta property="article:tag" content="paper review">
    
  


<meta name="twitter:card" content="summary">











  <meta property="og:image" content="https://www.gravatar.com/avatar/9f4c32e0b56b36cdb288d9b239c9324c?s=640">
  <meta property="twitter:image" content="https://www.gravatar.com/avatar/9f4c32e0b56b36cdb288d9b239c9324c?s=640">


    <title>[Paper review] TimeSformer</title>

    <link rel="icon" href="https://dj-woo.github.io/favicon.png">
    

    

    <link rel="canonical" href="https://dj-woo.github.io/2021/02/paper-review-timesformer/">

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/jquery.fancybox.min.css" integrity="sha256-vuXZ9LGmmwtjqFX1F+EKin1ThZMub58gKULUyf0qECk=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/helpers/jquery.fancybox-thumbs.min.css" integrity="sha256-SEa4XYAHihTcEP1f5gARTB2K26Uk8PsndQYHQC1f4jU=" crossorigin="anonymous" />
    
    
    <link rel="stylesheet" href="https://dj-woo.github.io/css/style-twzjdbqhmnnacqs0pwwdzcdbt8yhv8giawvjqjmyfoqnvazl0dalmnhdkvp7.min.css" />
    
    

    
      
    
    
  </head>

  <body>
    <div id="blog">
      <header id="header" data-behavior="4">
  <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
  <div class="header-title">
    <a class="header-title-link" href="https://dj-woo.github.io/">djwoo blog</a>
  </div>
  
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

</header>

      <nav id="sidebar" data-behavior="4">
  <div class="sidebar-container">
    
      <div class="sidebar-profile">
        <a href="https://dj-woo.github.io/#about">
          <img class="sidebar-profile-picture" src="https://www.gravatar.com/avatar/9f4c32e0b56b36cdb288d9b239c9324c?s=110" alt="Author&#39;s picture" />
        </a>
        <h4 class="sidebar-profile-name">Deumji Woo</h4>
        
      </div>
    
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://dj-woo.github.io/">
    
      <i class="sidebar-button-icon fa fa-lg fa-home"></i>
      
      <span class="sidebar-button-desc">Home</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://dj-woo.github.io/categories">
    
      <i class="sidebar-button-icon fa fa-lg fa-bookmark"></i>
      
      <span class="sidebar-button-desc">Categories</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://dj-woo.github.io/tags">
    
      <i class="sidebar-button-icon fa fa-lg fa-tags"></i>
      
      <span class="sidebar-button-desc">Tags</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://dj-woo.github.io/archives">
    
      <i class="sidebar-button-icon fa fa-lg fa-archive"></i>
      
      <span class="sidebar-button-desc">Archives</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://dj-woo.github.io/#about">
    
      <i class="sidebar-button-icon fa fa-lg fa-question"></i>
      
      <span class="sidebar-button-desc">About</span>
    </a>
  </li>


    </ul>
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://github.com/dj-woo" target="_blank" rel="noopener">
    
      <i class="sidebar-button-icon fa fa-lg fa-github"></i>
      
      <span class="sidebar-button-desc">GitHub</span>
    </a>
  </li>


    </ul>
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://dj-woo.github.io/index.xml">
    
      <i class="sidebar-button-icon fa fa-lg fa-rss"></i>
      
      <span class="sidebar-button-desc">RSS</span>
    </a>
  </li>


    </ul>
  </div>
</nav>

      

      <div id="main" data-behavior="4"
        class="
               hasCoverMetaIn
               ">
        <article class="post" itemscope itemType="http://schema.org/BlogPosting">
          
          
            <div class="post-header main-content-wrap text-left">
  
    <h1 class="post-title" itemprop="headline">
      [Paper review] TimeSformer
    </h1>
  
  
  <div class="postShorten-meta post-meta">
    
      <time itemprop="datePublished" datetime="2021-02-25T14:15:19&#43;09:00">
        
  February 25, 2021

      </time>
    
    
  
  
    <span>in</span>
    
      <a class="category-link" href="https://dj-woo.github.io/categories/paper-review">paper review</a>
    
  

  </div>

</div>
          
          <div class="post-content markdown" itemprop="articleBody">
            <div class="main-content-wrap">
              <p>[Paper review] <a href="https://arxiv.org/pdf/2102.05095v1.pdf">TimeSformer</a><br>
title   = Is Space-Time Attention All You Need for Video Understanding?<br>
author  = Gedas Bertasius and Heng Wang and Lorenzo Torresani<br>
year    = 9 Feb 2021</p>
<p>2020.2.9일에 나온 논문으로 Action Recognition &amp; Action Classfication task에서 상위에 rank되어 있습니다.
Video classfication에서 self-attention만을 활용한 <strong>TimeSformer</strong>를 제안하였으며, 아래와 같은 특징이 있습니다.</p>
<p>
</p>
<ul>
<li>conceptually simple</li>
<li>state-of-the-art results on major action recognition benchmarks</li>
<li>low inference cost</li>
<li>suitable for long-term video modeling</li>
</ul>
<h2 id="1-introduction">1. Introduction</h2>
<hr>
<p>Natural Language Processing(NLP) 분야에서는 self-attention을 사용한 <a href="https://arxiv.org/abs/1706.03762" title="option text">Transformer(2017)</a>가 나오면서, revolution이 있었습니다.
Transformer는 capturing long-range dependencies, training scalability에 장점이 있고, 이러한 Transformer를 사용한 다양한 방법이 NLP분야에서 나왔습니다.<br>
Video understanding의 경우, <strong>1.sequence data인 점, 2.문맥적 이해가 필요한 점</strong>에서 NLP와 비슷한 특징을 보이는데요.
이러한 특징을 기반으로 Transformer 기반의 long-range self-attention model이 video modeling에도 잘 작동할거라고 예상했습니다.
이를 통해, convolution operator가 내재적으로 가지고 있는 아래와 같은 제한 요소를 피할 수 있을거라 생각했습니다.</p>
<ol>
<li>CNN의 <strong>inductive biases</strong> (e.g., local connectivity and translation equivariance)은 small training set에서는 장점이 됩니다.
하지만, 데이터가 충분하여, 모든 걸 data에서 학습이 가능한 경우, model이 표현할 수 있는 범위를 제한할 수 있습니다.<br>
Transformer는 inductive biase가 상대적으로 적은 구조입니다.</li>
<li>CNN은 <strong>short-range spatiotemporal information capture</strong>에 특화된 구조입니다. 그래서 receptive field를 벗어난 영역에 대해서는 capture가 어렵습니다.
물론, layer를 deep하게 만들어서 receptive field를 넓힐 수는 있지만, layer를 넘는 과정에서 수행되는 mean과 같은 함수가 long-range dependency capture를 어렵게 합니다.<br>
Transformer는 pair-to-pair간 직접 비교를 하기 때문에, short-range와 long-range 모두 capture할 수 있습니다.</li>
<li>CNN 대비, Transformer가 <strong>GPU hardware acceleration</strong>에 더 적합한 구조를 가지고 있습니다.</li>
</ol>
<p>그래서 기존에 video modeling에서 주를 이루던 convolution operator를 사용하지 않고 self-attention만으로 이루어진 architecture를 제안합니다.</p>
<p><img src="https://dj-woo.github.io/img/TimeSformer/ViT.PNG" alt="Vision Transformer Architecture"></p>
<p>이를 위해, <strong>Vision Transformer</strong>를 확장하여, self-attention의 범위를 image space와 time space로 확장하였습니다.
제안하는 architecture인 <strong>TimeSformer(from Time-Space Transformer)는 video를 각 frame에서 뽑은 patch의 list 형태로 변환하여 처리</strong>합니다.</p>
<p>self-attention의 단점 중 하나가, pair-to-pari 연산을 위한 compution power인데요.
이를 극복하기 위한 방법으로, <strong>diveded attention</strong>을 사용한 방법도 같이 제안합니다.</p>
<h2 id="2-related-work">2. Related Work</h2>
<hr>
<p>기존에 나왔있던 방법들에 대해서 간단히 언급합니다.</p>
<ol>
<li>self-attention을 image classification에서 사용한 model</li>
<li>image networks leveraging에 self-attention을 사용한 model들
<ol>
<li>individual pixels를 사용한 model</li>
<li>full images를 사용하기 위해, sparse key-value sampling을 이용한 model</li>
<li>self-attention을 축 방향으로만 사용한 model (이 방법은 본논문에서도 일부 사용합니다.)</li>
</ol>
</li>
<li>Transformers를 CNN에서 나온 feature를 aggregation용으로 사용한 model들 (for action localization and recognition)</li>
<li>text Transformers과 video CNNs을 같이 사용한 model들 (for various video-language tasks)</li>
</ol>
<h2 id="3-the-timesformer-model">3. The TimeSformer Model</h2>
<hr>
<h3 id="input-clip">Input clip</h3>
<p>video에서 아래와 같은 형식으로 sampiling합니다.<br>
논문에서는 일반적으로 8x224×224x3을 사용하며, 추가적으로 high-resolution용으로 16×448×448x3를, long-range로 96×224×224x3를 사용합니다.<br>
<code>$$ X ∈ R^{FxH×W×3}\tag{0} $$</code></p>
<h3 id="decomposition-into-patches">Decomposition into patches</h3>
<p>ViT와 같이, 각 frame에서 non-overlapping으로 <code>$ P \times P$</code> size Path <code>$N$</code>개를 뽑습니다.<br>
이렇게 뽑은 patches를 vectors <code>$x_{(p,t)} ∈ R^{3P^2}$</code>로 flatten하게 정렬합니다.</p>
<ul>
<li><code>$p = 1, . . . , N$</code>는 하나의 frame상 spatial locations</li>
<li><code>$t = 1, . . . , F$</code>는 frame의 index</li>
</ul>
<h3 id="linear-embedding">Linear embedding</h3>
<p>path를 embedding vector로 project합니다.
<code>$$z^{(0)}_{(p,t)} = Ex_{(p,t)} + e^{pos}_{(p,t)}\tag{1} $$</code></p>
<ul>
<li>patch: <code>$x_{(p,t)}$</code></li>
<li>embedding vector: <code>$z^{(0)}_{(p,t)} ∈ R^D$</code></li>
<li>learnable matrix: <code>$E ∈ R^{D×3P^2}$</code></li>
<li>learnable positional embedding vectore: <code>$e^{pos}_{(p,t)} ∈ R^D$</code></li>
</ul>
<p>이때 BERT와 마찬가지로, sequence의 처음에는 classification을 위한 특별한 vecoter를 넣습니다.</p>
<h3 id="query-key-value-computation">Query-Key-Value computation</h3>
<p>TimeSformer에는 L개의 encoding block이 있으며, 각 block <code>$\ell$</code>의 query/key/value vector는 이전 layer<code>$\ell-1$</code>의 output patch에서 생성됩니다.</p>
<p><code>$$q^{(\ell,a)}_{(p,t)} = W^{(\ell,a)}_Q LN \bigg( z^{(\ell−1)}_{(p,t)} \bigg) ∈ R^{D_h} \tag{2} $$</code>
<code>$$k^{(\ell,a)}_{(p,t)} = W^{(\ell,a)}_K LN \bigg( z^{(\ell−1)}_{(p,t)} \bigg) ∈ R^{D_h} \tag{3} $$</code>
<code>$$v^{(\ell,a)}_{(p,t)} = W^{(\ell,a)}_V LN \bigg( z^{(\ell−1)}_{(p,t)} \bigg) ∈ R^{D_h} \tag{4} $$</code></p>
<ul>
<li><code>$LN()$</code>: LayerNorm</li>
<li><code>$a = 1, . . . , A$</code>: index over multiple attention heads</li>
<li><code>$A$</code>: the total number of attention heads</li>
<li><code>$D_h$</code> = <code>$D/A$</code></li>
</ul>
<h3 id="self-attention-computation">Self-attention computation</h3>
<p>Self-attention weights은 dot-product로 연산됩니다. self-attention weights<code>$α^{(\ell,a)}_{(p,t)} ∈ R^{NF+1}$</code>는 아래와 같이 계산됩니다.</p>
<p><code>$$α^{(\ell,a)}_{(p,t)} = SM \biggl( \frac{q^{(\ell,a)}_{(p,t)}}{\sqrt{D_h}}\ \cdot\ \biggl[ k^{(\ell,a)}_{(0,0)} \bigg\{ k^{(\ell,a)}_{(p^\prime,t^\prime)} \bigg\}_{p^\prime=1,...,N\\t^\prime=1,...,F} \biggl] \biggl) \tag{5} $$</code></p>
<ul>
<li><code>$SM()$</code>: softmax activation function</li>
</ul>
<p>식 (5)는 spatial-temporal 양방향 self-attention입니다.
이를 spatial-only로 제한하면, 계산량을 NxF+1에서 N+1로 줄일 수 있습니다.</p>
<h3 id="encoding">Encoding</h3>
<p>Layer <code>$\ell$</code>의 <code>$z^{(\ell)}_{(p,t)}$</code>는 아래와 같이 계산할 수 있습니다.
multi-head attention layer의 output <code>$s^{(\ell,a)}_{(p,t)}$</code>를 계산합니다.</p>
<p><code>$$s^{(\ell,a)}_{(p,t)} = α^{(\ell,a)}_{(p,t),(0,0)} v^{(\ell,a)}_{(0,0)} +\sum_{p^\prime=1}^N \sum_{t^\prime=1}^F α^{(\ell,a)}_{(p,t),(p^\prime,t^\prime)} v^{(\ell,a)}_{(p^\prime,t^\prime)} \tag{6}  $$</code></p>
<p>multi head의 output을 모두 concatation 후, projection합니다.
그후 residual connection을 추가하, LN과 MLP layer를 통과합니다.
그후 다시 residual connection을 추가합니다.</p>
<p><code>$${z^\prime}^{(\ell)}_{(p,t)} = W_O \biggl[ s^{(\ell,1)}_{(p,t)},...,s^{(\ell,A)}_{(p,t)}\biggl] +z^{(\ell-1)}_{(p,t)}\tag{7}$$</code>
<code>$$z^{(\ell)}_{(p,t)} = MLP(LN({z^\prime}^{(\ell)}_{(p,t)})) + {z^\prime}^{(\ell)}_{(p,t)}\tag{8} $$</code></p>
<p>이러한 encoder구조는 ViT와 동일합니다.</p>
<h3 id="classification-embedding">Classification embedding</h3>
<p>최종 classification은 마지막 encoding layer의 첫번째 token을 사용하여 진행합니다.
이때, MLP를 한단 더 추가하여, target class와 같은 size의 vectore를 생성합니다.</p>
<h3 id="space-time-self-attention-models">Space-Time Self-Attention Models</h3>
<p>앞에서 언급한것 처럼, spatiotempolar attention, 식(5),를 space-only 또는 time-only attention으로 변경하여 computation time을 줄일 수 있습니다.
하지만 이러한 방식은 다른 방향의 dependecy를 무시하여 accuracy를 저하시킵니다.</p>
<p>그래서 본 논문에서는 보다 효과적인 **Divided Space-Time Attention (denoted with T+S)**를 제안하였습니다.
이 방법은 tempolar-attenntion을 먼저 적용하고, spatial-attention을 나중에 적용하는 architecture입니다.
실험에서 사용한 다양한 attention layer 조합은 아래 그림에서 볼수 있습니다.</p>
<p><img src="https://dj-woo.github.io/img/TimeSformer/attention_types.PNG" alt="attention types"></p>
<p>이해를 위해서 이를 visualization하면, 아래와 같습니다.</p>
<p><img src="https://dj-woo.github.io/img/TimeSformer/visualization.PNG" alt="visualization"></p>
<p>각 architecture를 간단히 정리하면,</p>
<ul>
<li>Joint Space-Time Attention(ST)
<ul>
<li>spatiotempolar attention을 적용</li>
<li>computation per patch: NxF+1</li>
</ul>
</li>
<li>Divided Space-Time Attention(T+S)
<ul>
<li>tempolar-attention(F) 후, spatial-attention(N)를 수행</li>
<li>computation per patch: N+F+1</li>
</ul>
</li>
<li>Sparse Local Global Attention (L+G)
<ul>
<li>H/2 x W/2에 대한 spatiotempolar attention(local attention)(FxH/2xW/2) 후, 2 stride로 spatiotempolar attention(global aattention)(FxH/2xW/2)을 진행</li>
<li>computation per patch: FxN/2+1</li>
</ul>
</li>
<li>Axial Attention (T+W+H)
<ul>
<li>tempolar-attention(F) 진행후, W축 attention(W)와 H축 attention(H)를 진행</li>
<li>computation per patch: T+W+H</li>
</ul>
</li>
</ul>
<h2 id="4-experiment">4. Experiment</h2>
<hr>
<ul>
<li>Dataset(Acition recognition)
<ol>
<li>Kinetics-400 (Carreira &amp; Zisserman, 2017)</li>
<li>Kinetics-600 (Carreira et al., 2018)</li>
<li>Something-SomethingV2 (Goyal et al., 2017)</li>
<li>Diving-48 (Li et al., 2018)</li>
</ol>
</li>
<li>Base architecture
<ol>
<li>“Base” ViT model architecture (Dosovitskiy et al., 2020) pretrained on ImageNet (Russakovsky et al., 2014)</li>
</ol>
</li>
<li>Train
<ol>
<li>clip size(FxHxW): 8×224×224</li>
<li>frame sample rate: 1/16</li>
<li>patch size: 16 × 16</li>
</ol>
</li>
<li>Inference
<ol>
<li>sample a single temporal clip in the middle of the video</li>
<li>use 3 spatial crops (top-left, center, bottom-right)</li>
<li>final prediction: averaging the softmax scores of these 3 predictions.</li>
</ol>
</li>
</ul>
<h3 id="41--analysis-of-self-attention-schemes">4.1  Analysis of Self-Attention Schemes</h3>
<p><img src="https://dj-woo.github.io/img/TimeSformer/table1.PNG" alt="table1"></p>
<p>각 self-attention scheme들간 model size, 연상량, 성능을 비교하였습니다.</p>
<ol>
<li>Dataset별 중요 정보의 차이
<ul>
<li>K400의 경우, spatial information이 중요함</li>
<li>SSv2의 경우, 상대적으로 temporal information이 중요함.</li>
</ul>
</li>
<li>divided space-time attention의 결과가 joint space-time attention에 비해 더 좋음.
<ul>
<li>space-time attention의 learning-capacity(parameter 수)가 더 큼</li>
</ul>
</li>
</ol>
<p><img src="https://dj-woo.github.io/img/TimeSformer/figure3.PNG" alt="figure3"></p>
<p>divided space-time attention과 joint space-time attention의 scalability를 비교했습니다.
higher spatial resolution (left) 과 longer (right) videos에서 모두 divided space-time attention이 
더 큰 size에도 불구하고 더 좋은 scalability를 보여줍니다.
이후 실험부터, divided space-time attention을 default로 사용합니다.</p>
<h3 id="42-varying-the-number-of-tokens-in-space-and-time">4.2 Varying the Number of Tokens in Space and Time</h3>
<p><img src="https://dj-woo.github.io/img/TimeSformer/figure4.PNG" alt="figure4"></p>
<p>입력 token 수에 영향을 주는 요소는 spatial resolution을 늘려 N을 증가 시키거나, frame을 늘려 F를 증가시키는 것입니다.
위 그림과 같이, spatial resolution(up to certain point)과 frame 증가는 모두 performance의 증가를 보였습니다.
특히 frame의 경우, memory 제한으로 실험이 불가능한 96 frame까지 계속된 성능 향상을 보였습니다.
보통 CNN기반 video task에서 8~32 frames을 사용하는걸 고려하면, 96 frames은 매우 긴 시간입니다.</p>
<h3 id="43-the-importance-of-pretraining-and-dataset-scale">4.3 The Importance of Pretraining and Dataset Scale</h3>
<p>Transformer를 사용한 model의 경우, 매우 큰 dataset들로 train을 시켜야 결과가 좋게 나오는 경향이 있습니다.
이를 실험하기 위해, 우선 TimeSformer를 scratch부터 학습을 시키려고 하였지만, 잘 되지 않았다고 되어 있습니다.
그래서, 이후 실험에서는 모두 ImageNet으로 pre-training된 model을 사용하였습니다.
학습에 사용한 dataset의 크기별, 25%/50%/75%/100%, 성능은 아래 그림과 같습니다.</p>
<p><img src="https://dj-woo.github.io/img/TimeSformer/figure5.PNG" alt="figure5"></p>
<p>K400의 경우, 모든 subset에서 TimeSformer가 가장 좋은 성능을 보였습니다.
SSv5의 경우, 75%, 100%에서만 가장 좋은 성능을 보였습니다.
이러한 차이는, SSv5가 K400에 비해, 더 복잡한 tempolar pattern들이 있기 때문이라고 분석하였습니다.</p>
<h3 id="44-comparison-to-the-state-of-the-art">4.4 Comparison to the State-of-the-Art</h3>
<p>3가지 type의 TimeSformer와 기존 State-of-the-Art model들과 비교하였습니다.</p>
<ol>
<li><strong>TimeSformer</strong>
<ul>
<li>operating on 8 × 224 × 224 video clips</li>
</ul>
</li>
<li><strong>TimeSformer-HR</strong>
<ul>
<li>operating on 16 × 448 × 448 video clips</li>
</ul>
</li>
<li><strong>TimeSformer-L</strong>
<ul>
<li>operating on 96 × 224 × 224 video clips with 1/4 sampling</li>
</ul>
</li>
</ol>
<h4 id="kinetics-400">Kinetics-400</h4>
<p><img src="https://dj-woo.github.io/img/TimeSformer/table2.PNG" alt="table2"></p>
<p>위 Table은 K400 dataset에서의 정확도를 기존 model들과 비교하였습니다.
Inference시 사용한 input은 아래와 같이 moethod 마다 차이가 있습니다.</p>
<ul>
<li>Most previous methods: 10 temporal clips with 3 spatial crops (30 views)
<ul>
<li>shorter spatial side를 기준으로 scale 후, left-center-right 3 crops 사용</li>
</ul>
</li>
<li>TimeSformer: only 3 views (3 views)
<ul>
<li>top-left, center, bottom-right 3 crops 사용</li>
</ul>
</li>
</ul>
<p>이러한 views의 차이로 TimeSformer가 가장 적은 inference cost로 비교할만한 정확도를 얻었을 수 있었습니다.
또한 TimeSformer-L은 가장 좋은 정확도를 보였습니다.</p>
<p><img src="https://dj-woo.github.io/img/TimeSformer/figure6.PNG" alt="figure6"></p>
<p>inference에 사용하는 clips수가 성능에 미치는 영향은 위 그림을 통해서 볼수 있습니다.
TimeSformer-L의 경우, 1 clip이 전후 96frames(12sec in Kinetics video)를 cover하기 때문에, 1 clip에서도 충분히 좋은 성능을 보입니다.
한가지 아쉬운 부분은, 1 clip 처리를 위한 cost가 method마다 다르기 때문에, cost도 같이 보여주면 더 좋겠다는 생각이 듭니다.</p>
<p>추가하여, transfomer를 이용한 TimeSformer의 경우, CNN기반 model 대비 training 속도에서도 장점을 보입니다.
대략적으로 CNN에 비해, x3 배정도 training 속도가 빠릅니다.</p>
<ul>
<li>8×8 R50 on K400 takes 54.3 hours on 64 GPUs</li>
<li>I3D R50 under similar settings takes 45 hours using 32 GPUs</li>
<li>TimeSformer can be trained in 14.3 hours using 32 GPUs</li>
</ul>
<h4 id="kinetics-600">Kinetics-600</h4>
<p><img src="https://dj-woo.github.io/img/TimeSformer/table3.PNG" alt="table3"></p>
<p>k600 dataset에서도 k400과 비슷하게 좋은 결과를 보입니다.
한가지 특이한 점은, k400과 다르게 TimeSformer-HR이 가장 좋은 성능을 보입니다.</p>
<h4 id="something-something-v2--diving-48">Something-Something-V2 &amp; Diving-48</h4>
<p><img src="https://dj-woo.github.io/img/TimeSformer/table4.PNG" alt="table4"></p>
<p>temporally-heavy datasets인 SSv2와 Diving-48에서의 결과입니다.</p>
<h3 id="45-long-term-video-modeling">4.5. Long-Term Video Modeling</h3>
<p>조금 더 긴 term의 video에 대한 성능 비교를 위해서, HowTo100M dataset의 sub-set을 random하게 만들어 사용하였습니다.
HowTo100M은 평균 7분정도의 길이를 가지고 있습니다.</p>
<p><img src="https://dj-woo.github.io/img/TimeSformer/table5.PNG" alt="table5"></p>
<p>table에서 SlowFast는 2 경우 모두 32 frames을 input으로 사용하였습니다.
다른 점은, sampling 주기로, 각각 1/8(1초에 4개 정도) 그리고 1/32(1초에 1개 정도)를 사용하였습니다.
TimeSformer의 경우, sampling 주기는 모두 1/32로 동일하게 하고, 대신 input frame을 가변하였습니다.
clips수는 video full로 cover할수 있도록 뽑았으며, 최종 classification은 각 clip 결과를 평균하였습니다.</p>
<p>같은 single-clip coverage model을 비교시 7~8% 정도 좋은 성능을 보였습니다.
96 frames을 inpput으로 받은 TimeSformer이 가장 좋은 성능을 보였습니다.</p>
<p>이를 통해, TimeSformer이 long-range video modeling에 좋은 구조이며,
video feature 추출을 위한 model로 사용하기 좋다고 결론 짓습니다.</p>
<h2 id="5-conclusion">5. Conclusion</h2>
<p>이전까지 video modeling 방법들, convolution-based video networks,과 다른 self-attention에 기반을 둔 TimeSformer를 제안하였습니다.
TimeSformer의 장점은 simple한 구조와 low inference cost 그리고 long-term video modeling에 강한다는 것입니다.</p>
<p>하지만 상대적으로 temporally-heavy dataset에서는 좋은 성능을 보여주지 못했습니다.
개인적으로 tempolar domain을 많이 희생해서, computation cost를 감소 시켰다고 생각합니다.</p>
<h2 id="reference">Reference</h2>
<hr>
<ul>
<li><a href="https://arxiv.org/abs/1706.03762" title="option text">Attention Is All You Need</a></li>
<li><a href="https://arxiv.org/pdf/2010.11929.pdf" title="option text">AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE</a></li>
<li><a href="https://jeonsworld.github.io/vision/vit" title="option text">[논문리뷰] An Image is Worth 16X16 Words: Transformers for Image Recognition at Scale</a></li>
<li><a href="https://arxiv.org/pdf/1812.03982" title="option text">SlowFast Networks for Video Recognition</a></li>
<li><a href="https://chacha95.github.io/2019-07-20-VideoUnderstanding6" title="option text">SlowFast Networks 리뷰</a></li>
</ul>
<!-- raw HTML omitted -->
              
            </div>
          </div>
          <div id="post-footer" class="post-footer main-content-wrap">
            
              
                
                
                  <div class="post-footer-tags">
                    <span class="text-color-light text-small">TAGGED IN</span><br/>
                    
  <a class="tag tag--primary tag--small" href="https://dj-woo.github.io/tags/paper-review/">paper review</a>

                  </div>
                
              
            
            <div class="post-actions-wrap">
  
      <nav >
        <ul class="post-actions post-action-nav">
          
            <li class="post-action">
              
                <a class="post-action-btn btn btn--disabled">
              
                  <i class="fa fa-angle-left"></i>
                  <span class="hide-xs hide-sm text-small icon-ml">NEXT</span>
                </a>
            </li>
            <li class="post-action">
              
                <a class="post-action-btn btn btn--default tooltip--top" href="https://dj-woo.github.io/2021/02/i3d/" data-tooltip="I3D">
              
                  <span class="hide-xs hide-sm text-small icon-mr">PREVIOUS</span>
                  <i class="fa fa-angle-right"></i>
                </a>
            </li>
          
        </ul>
      </nav>
    <ul class="post-actions post-action-share" >
      
        <li class="post-action hide-lg hide-md hide-sm">
          <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions">
            <i class="fa fa-share-alt"></i>
          </a>
        </li>
        
      
      
      <li class="post-action">
        
          <a class="post-action-btn btn btn--default" href="#">
        
          <i class="fa fa-list"></i>
        </a>
      </li>
    </ul>
  
</div>

            
              
            
          </div>
        </article>
        <footer id="footer" class="main-content-wrap">
  <span class="copyrights">
    &copy; 2021 Deumji Woo. All Rights Reserved
  </span>
</footer>

      </div>
      <div id="bottom-bar" class="post-bottom-bar" data-behavior="4">
        <div class="post-actions-wrap">
  
      <nav >
        <ul class="post-actions post-action-nav">
          
            <li class="post-action">
              
                <a class="post-action-btn btn btn--disabled">
              
                  <i class="fa fa-angle-left"></i>
                  <span class="hide-xs hide-sm text-small icon-ml">NEXT</span>
                </a>
            </li>
            <li class="post-action">
              
                <a class="post-action-btn btn btn--default tooltip--top" href="https://dj-woo.github.io/2021/02/i3d/" data-tooltip="I3D">
              
                  <span class="hide-xs hide-sm text-small icon-mr">PREVIOUS</span>
                  <i class="fa fa-angle-right"></i>
                </a>
            </li>
          
        </ul>
      </nav>
    <ul class="post-actions post-action-share" >
      
        <li class="post-action hide-lg hide-md hide-sm">
          <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions">
            <i class="fa fa-share-alt"></i>
          </a>
        </li>
        
      
      
      <li class="post-action">
        
          <a class="post-action-btn btn btn--default" href="#">
        
          <i class="fa fa-list"></i>
        </a>
      </li>
    </ul>
  
</div>

      </div>
      <div id="share-options-bar" class="share-options-bar" data-behavior="4">
  <i id="btn-close-shareoptions" class="fa fa-close"></i>
  <ul class="share-options">
    
  </ul>
</div>
<div id="share-options-mask" class="share-options-mask"></div>
    </div>
    
    <div id="about">
  <div id="about-card">
    <div id="about-btn-close">
      <i class="fa fa-remove"></i>
    </div>
    
      <img id="about-card-picture" src="https://www.gravatar.com/avatar/9f4c32e0b56b36cdb288d9b239c9324c?s=110" alt="Author&#39;s picture" />
    
    <h4 id="about-card-name">Deumji Woo</h4>
    
    
    
      <div id="about-card-location">
        <i class="fa fa-map-marker"></i>
        <br/>
        Korea
      </div>
    
  </div>
</div>

    

    
  
    
      <div id="cover" style="background-image:url('https://dj-woo.github.io/images/cover.jpg');"></div>
    
  


    
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js" integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin="anonymous"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.7/js/jquery.fancybox.min.js" integrity="sha256-GEAnjcTqVP+vBp3SSc8bEDQqvWAZMiHyUSIorrWwH50=" crossorigin="anonymous"></script>


<script src="https://dj-woo.github.io/js/script-pcw6v3xilnxydl1vddzazdverrnn9ctynvnxgwho987mfyqkuylcb1nlt.min.js"></script>


<script lang="javascript">
window.onload = updateMinWidth;
window.onresize = updateMinWidth;
document.getElementById("sidebar").addEventListener("transitionend", updateMinWidth);
function updateMinWidth() {
  var sidebar = document.getElementById("sidebar");
  var main = document.getElementById("main");
  main.style.minWidth = "";
  var w1 = getComputedStyle(main).getPropertyValue("min-width");
  var w2 = getComputedStyle(sidebar).getPropertyValue("width");
  var w3 = getComputedStyle(sidebar).getPropertyValue("left");
  main.style.minWidth = `calc(${w1} - ${w2} - ${w3})`;
}
</script>

<script>
$(document).ready(function() {
  hljs.configure({ classPrefix: '', useBR: false });
  $('pre.code-highlight > code, pre > code').each(function(i, block) {
    if (!$(this).hasClass('codeblock')) {
      $(this).addClass('codeblock');
    }
    hljs.highlightBlock(block);
  });
});
</script>


  
    
  




    
  </body>
</html>

