<!DOCTYPE html>
<html lang="ko-kr">
  <head>
    
    <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="generator" content="Hugo 0.65.0 with theme Tranquilpeak 0.4.8-BETA">
<meta name="author" content="Deumji Woo">
<meta name="keywords" content="">
<meta name="description" content="">


<meta property="og:description" content="">
<meta property="og:type" content="article">
<meta property="og:title" content="Transformer">
<meta name="twitter:title" content="Transformer">
<meta property="og:url" content="https://dj-woo.github.io/2020/09/transformer/">
<meta property="twitter:url" content="https://dj-woo.github.io/2020/09/transformer/">
<meta property="og:site_name" content="djwoo blog">
<meta property="og:description" content="">
<meta name="twitter:description" content="">
<meta property="og:locale" content="en-us">

  
    <meta property="article:published_time" content="2020-09-13T17:00:49">
  
  
    <meta property="article:modified_time" content="2020-09-13T17:00:49">
  
  
  
    
      <meta property="article:section" content="ML">
    
  
  
    
      <meta property="article:tag" content="ML">
    
      <meta property="article:tag" content="paper review">
    
  


<meta name="twitter:card" content="summary">











  <meta property="og:image" content="https://www.gravatar.com/avatar/9f4c32e0b56b36cdb288d9b239c9324c?s=640">
  <meta property="twitter:image" content="https://www.gravatar.com/avatar/9f4c32e0b56b36cdb288d9b239c9324c?s=640">


    <title>Transformer</title>

    <link rel="icon" href="https://dj-woo.github.io/favicon.png">
    

    

    <link rel="canonical" href="https://dj-woo.github.io/2020/09/transformer/">

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/jquery.fancybox.min.css" integrity="sha256-vuXZ9LGmmwtjqFX1F+EKin1ThZMub58gKULUyf0qECk=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/helpers/jquery.fancybox-thumbs.min.css" integrity="sha256-SEa4XYAHihTcEP1f5gARTB2K26Uk8PsndQYHQC1f4jU=" crossorigin="anonymous" />
    
    
    <link rel="stylesheet" href="https://dj-woo.github.io/css/style-twzjdbqhmnnacqs0pwwdzcdbt8yhv8giawvjqjmyfoqnvazl0dalmnhdkvp7.min.css" />
    
    

    
      
    
    
  </head>

  <body>
    <div id="blog">
      <header id="header" data-behavior="4">
  <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
  <div class="header-title">
    <a class="header-title-link" href="https://dj-woo.github.io/">djwoo blog</a>
  </div>
  
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

</header>

      <nav id="sidebar" data-behavior="4">
  <div class="sidebar-container">
    
      <div class="sidebar-profile">
        <a href="https://dj-woo.github.io/#about">
          <img class="sidebar-profile-picture" src="https://www.gravatar.com/avatar/9f4c32e0b56b36cdb288d9b239c9324c?s=110" alt="Author&#39;s picture" />
        </a>
        <h4 class="sidebar-profile-name">Deumji Woo</h4>
        
      </div>
    
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://dj-woo.github.io/">
    
      <i class="sidebar-button-icon fa fa-lg fa-home"></i>
      
      <span class="sidebar-button-desc">Home</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://dj-woo.github.io/categories">
    
      <i class="sidebar-button-icon fa fa-lg fa-bookmark"></i>
      
      <span class="sidebar-button-desc">Categories</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://dj-woo.github.io/tags">
    
      <i class="sidebar-button-icon fa fa-lg fa-tags"></i>
      
      <span class="sidebar-button-desc">Tags</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://dj-woo.github.io/archives">
    
      <i class="sidebar-button-icon fa fa-lg fa-archive"></i>
      
      <span class="sidebar-button-desc">Archives</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://dj-woo.github.io/#about">
    
      <i class="sidebar-button-icon fa fa-lg fa-question"></i>
      
      <span class="sidebar-button-desc">About</span>
    </a>
  </li>


    </ul>
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://github.com/dj-woo" target="_blank" rel="noopener">
    
      <i class="sidebar-button-icon fa fa-lg fa-github"></i>
      
      <span class="sidebar-button-desc">GitHub</span>
    </a>
  </li>


    </ul>
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://dj-woo.github.io/index.xml">
    
      <i class="sidebar-button-icon fa fa-lg fa-rss"></i>
      
      <span class="sidebar-button-desc">RSS</span>
    </a>
  </li>


    </ul>
  </div>
</nav>

      

      <div id="main" data-behavior="4"
        class="
               hasCoverMetaIn
               ">
        <article class="post" itemscope itemType="http://schema.org/BlogPosting">
          
          
            <div class="post-header main-content-wrap text-left">
  
    <h1 class="post-title" itemprop="headline">
      Transformer
    </h1>
  
  
  <div class="postShorten-meta post-meta">
    
      <time itemprop="datePublished" datetime="2020-09-13T17:00:49&#43;09:00">
        
  September 13, 2020

      </time>
    
    
  
  
    <span>in</span>
    
      <a class="category-link" href="https://dj-woo.github.io/categories/ml">ML</a>
    
  

  </div>

</div>
          
          <div class="post-content markdown" itemprop="articleBody">
            <div class="main-content-wrap">
              <p><strong>개인 study를 위한 자료입니다.</strong><br>
<strong>그러다보니 내용에 잘못된 점이 있습니다.</strong></p>
<h1 id="1introduction--2background">1.Introduction &amp; 2.Background</h1>
<p>기존 방법들에 대한 언급과 장단점을 기술하였습니다.
실제 transformer 구현에 대한 설명은 3절부터 시작됩니다.</p>
<h1 id="3-model-architecture">3. Model Architecture</h1>
<ul>
<li>transformer의 model architecture는 아래와 같습니다.
<img src="https://dj-woo.github.io/img/transformer/model_architecture.PNG" alt="Model Architecture">
in/out 관계
<ul>
<li>x = an input sequence of symbol representations</li>
<li>z = a sequence of continuous representations</li>
<li>y = an output sequence</li>
<li>z = encoder(x)</li>
<li>y = decoder(z)</li>
</ul>
</li>
</ul>
<h2 id="31-encoder-and-decoder-stacks">3.1 Encoder and Decoder Stacks</h2>
<ul>
<li>
<p>Encoder: 2개의 sub-layer로 구성됨.</p>
<ol>
<li>multi-head (self) attention</li>
<li>(fully connected) feed-forward network</li>
</ol>
</li>
<li>
<p>Decoder: 3개의 sub-layer로 구성됨.</p>
<ol>
<li>2개의 sub-layer는 encoder와 같음.</li>
<li>masked multi-head attention: multi-head attention에 decoder의 특성을 반영하기 위해 mask를 추가하였습니다.</li>
</ol>
<blockquote>
<p>predictions for position i can depend only on the known outputs at positions less than i.</p>
</blockquote>
</li>
</ul>
<p>각 sub-layer에는 residual connection이 있고, 뒤에 normalization layer가 붙어 있습니다.<br>
따라서, 각 sub-layer의 output은 <code>LayerNorm(x + Sublayer(x))</code> 입니다.
이 절에서 논문에 전의된 변수는 아래와 같습니다.</p>
<ul>
<li>Encoder: stack N = 6, d_model = 512</li>
<li>Decoder: stack N = 6</li>
</ul>
<h2 id="32-in-attention">3.2 In Attention</h2>
<h3 id="321-scaled-dot-product-attention">3.2.1 Scaled Dot-Product Attention</h3>
<p><img src="https://dj-woo.github.io/img/transformer/scaled_dot_product_attention.PNG" alt="Model Architecture">
Input은 Query, Key 그리고 value로 구성되어 있으면, 각각 아래의 size를 가집니다.</p>
<ul>
<li>Q, K = <code>$d_k$</code></li>
<li>V = <code>$d_v$</code> = <code>$d_{model}$</code></li>
</ul>
<p>Scaled Dot-Product attention layer는 위에 그림처럼, <code>$QK^T$</code>에 softmax를 취하여 weight을 계산합니다.
그리고, 이 값을 Value와 곱하는 layer입니다. 전체 식은 아래와 같습니다.<br>
<code>$$ Attention(Q,K,V) = {softmax(\frac{QK^T}{\sqrt{d_k}})V} $$</code><br>
여기서 <code>$\sqrt{d_k}$</code>로 나누는 이유는 <code>$\sqrt{d_k}</code>의 값이 거질수로, dot-product의 결과값이 softmax가 잘 동작하지 않는 regions으로 가기 때문입니다.</p>
<blockquote>
<p><strong>paper</strong>: To illustrate why the dot products get large, assume that the components of q and k are independent random
variables with mean 0 and variance 1. Then their dot product, <code>${qk} = {\sum_{i=1}^{d_k}q_iK_i}$</code>, has mean 0 and variance <code>$d_k$</code>.</p>
</blockquote>
<h3 id="322-multi-head-attention">3.2.2 Multi-head Attention</h3>
<p><img src="https://dj-woo.github.io/img/transformer/multi_head_attention.PNG" alt="Model Architecture">
<code>$d_{model}$</code>의 K,Q,V에 하나의 attention을 적용하는 것보다, 각각을 h 번 서로 다른 <code>$d_{k}$</code>,<code>$d_{q}$</code>,<code>$d_{v}$</code>로 linear project 하여, 
각각에 attention function을 병렬적으로 수행하는게 더 좋다는 것을 발견했다고 논문에 나와 있습니다. h개의 <code>$d_{v}$</code>을 concat하고 한번 더 project
하면 input과 동일한 값을 얻을 수 있습니다. 식으로 나타내면 아래와 같습니다.
<code>$$ MultiHead(Q,K,V) = { Concat(head_1,....,head_h)W^O}\\ where\ {head_i} = Attention(QW_i^Q, KW_i^K, VW_i^K)$$</code><br>
<code>$W_i^Q, W_i^K, W_i^K$</code>는 각각 <code>$d_{model}$</code>의 input을 <code>$d_q, d_k, d_v$</code>로 linear project 시켜줍니다.
논문에서는 사용한 값은 아래와 같습니다. h개의 head를 사용하지만, dimension을 1/h로 줄였기 때문에, total computational cost는
single head를 사용할때와 유사합니다.<br>
<code>$ h = 8, d_k=d_q=d_v=d_{model}/h=64$</code></p>
<blockquote>
<p><strong>paper</strong>: we found it beneficial to linearly project the queries, keys and values h times with different, learned
linear projections to dk, dk and dv dimensions, respectively.<br>
<strong>paper</strong>: Due to the reduced dimension of each head, the total computational cost
is similar to that of single-head attention with full dimensionality.</p>
</blockquote>
<h3 id="323-applications-of-attention-in-our-model">3.2.3 Applications of Attention in our Model</h3>
<p>앞에 내용을 정리해서, 적용한 attention model의 특징을 3가지로 설명 합니다.</p>
<ul>
<li>&ldquo;encoder-decoder attention layer&quot;에서 query는 앞단의 decoder layer에서 오고, key와 value는 encoder의 저장된 output입니다. 
이렇게 해서, decoder의 모든 position에서 input sequence의 모든 position을 참조 할 수 있습니다.<br>
이때까지, 앞단의 decoder에서 받는 값은 value라고 생각했는데, query 네요..</li>
<li>encoder는 self-attention layer를 사용합니다. encoder의 각 position을 계산할때는, 모든 position을 참조합니다.</li>
<li>encoder와 비슷하게, decoder도 self-attention layer가 있습니다. 다른 점은, decoder의 각 position을 계산할때는
해당 position과 그 이전 position까지의 data만 사용할 수 있습니다.</li>
</ul>
<h2 id="33-position-wise-feed-forward-networks">3.3 Position-wise Feed-Forward Networks</h2>
<p><img src="https://dj-woo.github.io/img/transformer/feedforward.PNG" alt="FeedForward">
Feedforward layer는 2단의 fully connected layer가 activation function으로 연결된 형태입니다.
input/output의 dimension은 <code>$d_{model} = 512$</code>이지만, inner-layer는 4배 많은 <code>$d_{ff}=2048$</code>을 사용합니다.</p>
<ul>
<li>size(W1) = <code>$d_{model}\ \times\ d_{ff} = 512 \times 2048 $</code></li>
<li>size(W2) = <code>$d_{ff}\ \times\ d_{model} = 2048 \times 512 $</code></li>
</ul>
<h2 id="34-embeddings-and-softmax">3.4 Embeddings and Softmax</h2>
<p>input/output 문장(<code>$[128\ \times\ 1]$</code>)이 주어지면, 한 단어를 뜻하는 token을 vector(<code>$[1\ \times\ $d_{model}]$</code>)로 변환해주는 embedding을 합니다.
논문에서는 미리 학습된 embedding 을 사용했다고 나와있습니다. decoder의 output을 처리하기 위해서도 비슷한 과정이 필요합니다.
embedding의 역행렬 개념인 pre-softmax linear transformation을 하고, softmax function을 통해서, 
decoder output을 next-token의 확률로 변환합니다. 
input/output을 처리하기 위한 embeding과 pre-softmax linear transformation에는 동일한 weight matrix를 사용했습니다.
그리고 embedding layer에는 <code>$\sqrt{d_{model}}$</code>를 weight에 곱했습니다.(????)</p>
<h2 id="35-positional-encoding">3.5 Positional Encoding</h2>
<p>recurrence, convolution을 사용하지 않았기 때문에, sequence의 order를 model이 학습할 수 있도록, position에 대한 값을 추가해야합니다.
이를 위해서, embedding step에 &ldquo;positional encoding&quot;을 추가했습니다. 
positional encoding도 embedding과 동일한 dimension <code>$d_{model}$</code>을 가지기 때문에, 2 값을 합이 가능합니다.
positional encoding을 위해서 사용한 식은 아래와 같습니다.
<code>$$ PE_{(pos,2i)} = sin(pos / 1000^{2i/d_{model}})\\ PE_{(pos,2i+1)} = cos(pos / 1000^{2i/d_{model}}) $$</code>
이 식을 선택한 이유는, pos값에 상관없이 pos+k와 pos간의 관계가 linear하기 때문에, model이 relative position관계를 학습하기 쉽기 때문입니다.</p>
<blockquote>
<p><strong>paper</strong>:  We chose this function because we hypothesized it would allow the model to easily learn to attend by
relative positions, since for any fixed offset k, <code>$PE_{pos+k}$</code> can be represented as a linear function of <code>$PE_{pos}$</code>.</p>
</blockquote>
<h1 id="4-why-self-attention">4 Why Self-Attention</h1>
<p><img src="https://dj-woo.github.io/img/transformer/table1.PNG" alt="table1"></p>
<ul>
<li>n = sequence length, d = representation dimension, k = kernel size of convolutions, r = size of the neighborhood in restricted self-attention.</li>
</ul>
<p>본 논문에서는 self-attention layer를 사용하여 x를 동일한 길이의 sequence vector z에 mapping 하였습니다.
self-attention layer를 사용한 이유로 recurrent layer와 convolution layer와 비교하여 아래 3가지를 들었습니다.</p>
<ol>
<li>complexity per layer</li>
<li>sequential operations</li>
<li>path length between long-range dependencies (Maximum Path Length)</li>
</ol>
<p>3번 Maximum PaTH Length는 이해하기가 그나마 쉽습니다.<br>
2번은 Convolutional layer가 <code>$O(log_k(n)$</code>가 되야 할것 같은데, <code>$O(1)$</code>인 이유를 잘 모르겠습니다.
1번은 self-attention만 한 단위 계산 복잡도가 <code>$d$</code>이고 recurent와 convolutional은 <code>$d^2$</code>인데, 이 이유를 잘 모르겠습니다.</p>
<p>그외에 side-benefit으로 interpretable models이라는 점을 들었습니다.</p>
<h1 id="5-training">5 Training</h1>
<p>실제 training 시킨 조건에 대한 설명이 있습니다. 몇몇 부분이 구현과 관련있습니다.</p>
<h1 id="53-optimizer">5.3 Optimizer</h1>
<p>Adam optimizer를 사용했습니다.</p>
<h1 id="54-regularization">5.4 Regularization</h1>
<p>3가지 type(?)의 regularization을 사용했습니다.</p>
<ol>
<li>Residual Dropout: 각 sub-layer output 에 사용 (Pdrop = 0.1)</li>
<li>Residual Dropout: embedding + positional embedding 결과 값에 사용 (Pdrop = 0.1)</li>
<li>Label Smoothing: 이 방법은 model 이 확실한 결과값을 학습하지 못하게 하지만,
BLEU score에 도움이 되었다고 합니다.</li>
</ol>
<h1 id="62-model-variations">6.2 Model Variations</h1>
<p>hyper parameter를 변경하면서 performance를 비교하였습니다.
model에서 사용한 hyper parameter가 어떤 것들이 있는지 한눈에 보기 좋습니다.</p>
<p><img src="https://dj-woo.github.io/img/transformer/table3.PNG" alt="table3"></p>
<p>(A): header 수에 변화를 주었습니다. 너무 많아도 성능이 저하 됩니다.<br>
(B): key dimension 에 작으면 성능이 나빠집니다.<br>
(C): 예상과 같이, &ldquo;bigger is better&rdquo; 임을 보여주는 결과 입니다.<br>
(D): drop-out 은 꼭 해야함.<br>
(E): positional encoding 을 미리 학습된 model를 사용한 겱과입니다. 결과는 큰 차이없습니다.</p>
<h1 id="reference">Reference</h1>
<ul>
<li><a href="https://paul-hyun.github.io/transformer-01/" title="option text">Transformer (Attention Is All You Need) 구현하기 (1/3)</a></li>
</ul>
<!-- raw HTML omitted -->
              
            </div>
          </div>
          <div id="post-footer" class="post-footer main-content-wrap">
            
              
                
                
                  <div class="post-footer-tags">
                    <span class="text-color-light text-small">TAGGED IN</span><br/>
                    
  <a class="tag tag--primary tag--small" href="https://dj-woo.github.io/tags/ml/">ML</a>

  <a class="tag tag--primary tag--small" href="https://dj-woo.github.io/tags/paper-review/">paper review</a>

                  </div>
                
              
            
            <div class="post-actions-wrap">
  
      <nav >
        <ul class="post-actions post-action-nav">
          
            <li class="post-action">
              
                <a class="post-action-btn btn btn--default tooltip--top" href="https://dj-woo.github.io/2020/10/torch_tutorials/" data-tooltip="Torch_tutorials">
              
                  <i class="fa fa-angle-left"></i>
                  <span class="hide-xs hide-sm text-small icon-ml">NEXT</span>
                </a>
            </li>
            <li class="post-action">
              
                <a class="post-action-btn btn btn--disabled">
              
                  <span class="hide-xs hide-sm text-small icon-mr">PREVIOUS</span>
                  <i class="fa fa-angle-right"></i>
                </a>
            </li>
          
        </ul>
      </nav>
    <ul class="post-actions post-action-share" >
      
        <li class="post-action hide-lg hide-md hide-sm">
          <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions">
            <i class="fa fa-share-alt"></i>
          </a>
        </li>
        
      
      
      <li class="post-action">
        
          <a class="post-action-btn btn btn--default" href="#">
        
          <i class="fa fa-list"></i>
        </a>
      </li>
    </ul>
  
</div>

            
              
            
          </div>
        </article>
        <footer id="footer" class="main-content-wrap">
  <span class="copyrights">
    &copy; 2020 Deumji Woo. All Rights Reserved
  </span>
</footer>

      </div>
      <div id="bottom-bar" class="post-bottom-bar" data-behavior="4">
        <div class="post-actions-wrap">
  
      <nav >
        <ul class="post-actions post-action-nav">
          
            <li class="post-action">
              
                <a class="post-action-btn btn btn--default tooltip--top" href="https://dj-woo.github.io/2020/10/torch_tutorials/" data-tooltip="Torch_tutorials">
              
                  <i class="fa fa-angle-left"></i>
                  <span class="hide-xs hide-sm text-small icon-ml">NEXT</span>
                </a>
            </li>
            <li class="post-action">
              
                <a class="post-action-btn btn btn--disabled">
              
                  <span class="hide-xs hide-sm text-small icon-mr">PREVIOUS</span>
                  <i class="fa fa-angle-right"></i>
                </a>
            </li>
          
        </ul>
      </nav>
    <ul class="post-actions post-action-share" >
      
        <li class="post-action hide-lg hide-md hide-sm">
          <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions">
            <i class="fa fa-share-alt"></i>
          </a>
        </li>
        
      
      
      <li class="post-action">
        
          <a class="post-action-btn btn btn--default" href="#">
        
          <i class="fa fa-list"></i>
        </a>
      </li>
    </ul>
  
</div>

      </div>
      <div id="share-options-bar" class="share-options-bar" data-behavior="4">
  <i id="btn-close-shareoptions" class="fa fa-close"></i>
  <ul class="share-options">
    
  </ul>
</div>
<div id="share-options-mask" class="share-options-mask"></div>
    </div>
    
    <div id="about">
  <div id="about-card">
    <div id="about-btn-close">
      <i class="fa fa-remove"></i>
    </div>
    
      <img id="about-card-picture" src="https://www.gravatar.com/avatar/9f4c32e0b56b36cdb288d9b239c9324c?s=110" alt="Author&#39;s picture" />
    
    <h4 id="about-card-name">Deumji Woo</h4>
    
    
    
      <div id="about-card-location">
        <i class="fa fa-map-marker"></i>
        <br/>
        Korea
      </div>
    
  </div>
</div>

    

    
  
    
      <div id="cover" style="background-image:url('https://dj-woo.github.io/images/cover.jpg');"></div>
    
  


    
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js" integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin="anonymous"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.7/js/jquery.fancybox.min.js" integrity="sha256-GEAnjcTqVP+vBp3SSc8bEDQqvWAZMiHyUSIorrWwH50=" crossorigin="anonymous"></script>


<script src="https://dj-woo.github.io/js/script-pcw6v3xilnxydl1vddzazdverrnn9ctynvnxgwho987mfyqkuylcb1nlt.min.js"></script>


<script lang="javascript">
window.onload = updateMinWidth;
window.onresize = updateMinWidth;
document.getElementById("sidebar").addEventListener("transitionend", updateMinWidth);
function updateMinWidth() {
  var sidebar = document.getElementById("sidebar");
  var main = document.getElementById("main");
  main.style.minWidth = "";
  var w1 = getComputedStyle(main).getPropertyValue("min-width");
  var w2 = getComputedStyle(sidebar).getPropertyValue("width");
  var w3 = getComputedStyle(sidebar).getPropertyValue("left");
  main.style.minWidth = `calc(${w1} - ${w2} - ${w3})`;
}
</script>

<script>
$(document).ready(function() {
  hljs.configure({ classPrefix: '', useBR: false });
  $('pre.code-highlight > code, pre > code').each(function(i, block) {
    if (!$(this).hasClass('codeblock')) {
      $(this).addClass('codeblock');
    }
    hljs.highlightBlock(block);
  });
});
</script>


  
    
  




    
  </body>
</html>

